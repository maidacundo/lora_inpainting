{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "realistic_vision_path = hf_hub_download(repo_id=\"SG161222/Realistic_Vision_V5.1_noVAE\", filename=\"Realistic_Vision_V5.1-inpainting.safetensors\")\n",
    "vae_path = hf_hub_download(repo_id=\"stabilityai/sd-vae-ft-mse-original\", filename=\"vae-ft-mse-840000-ema-pruned.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import DatasetConfig, Config, ModelConfig, WandbConfig, EvaluationConfig, TrainConfig\n",
    "\n",
    "dataset_config = DatasetConfig(\n",
    "    roboflow_api_key='HNXIsW3WwnidNDQZHexX',\n",
    "    roboflow_workspace='arked',\n",
    "    project_name='facades-flzke',\n",
    "    dataset_version=11,\n",
    "    data_root='facades_data',\n",
    "    image_size=512,\n",
    ")\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    model_path=realistic_vision_path,\n",
    "    vae_path=vae_path,\n",
    ")\n",
    "\n",
    "wandb_config = WandbConfig(\n",
    "    project_name='facades',\n",
    ")\n",
    "\n",
    "eval_config=EvaluationConfig(\n",
    "    prompts=['white facade', 'brick facade'],\n",
    ")\n",
    "\n",
    "train_config=TrainConfig(\n",
    "    checkpoint_folder = wandb_config.project_name + \"_checkpoints\",\n",
    "    train_batch_size = 4,\n",
    "    unet_lr=1e-4,\n",
    "    text_encoder_lr=1e-4,\n",
    "    scheduler_num_cycles=4,\n",
    "    total_steps=1000,\n",
    ")\n",
    "\n",
    "config = Config(\n",
    "    dataset=dataset_config,\n",
    "    model=model_config,\n",
    "    wandb=wandb_config,\n",
    "    eval=eval_config,\n",
    "    train=train_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in facades_data to yolov7pytorch:: 100%|██████████| 12056/12056 [00:00<00:00, 14370.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to facades_data in yolov7pytorch:: 100%|██████████| 219/219 [00:00<00:00, 1286.99it/s]\n"
     ]
    }
   ],
   "source": [
    "download_roboflow_dataset(config)\n",
    "set_random_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VAE...\n",
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n"
     ]
    }
   ],
   "source": [
    "from model import get_models\n",
    "\n",
    "text_encoder, vae, unet, tokenizer, noise_scheduler, placeholder_token_ids = get_models(\n",
    "    config.model.model_path,\n",
    "    config.model.vae_path,\n",
    "    device=config.device,\n",
    "    load_from_safetensor=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import wandb\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.checkpoint\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from diffusers import StableDiffusionInpaintPipeline, logging\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from diffusers.optimization import get_scheduler\n",
    "from peft import LoraConfig, LoraModel\n",
    "from controlnet_aux import MLSDdetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from roboflow import Roboflow\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2 as transforms\n",
    "from torchvision.transforms.v2 import functional as TF\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from utils import parse_labels, scale_polygons, generate_masks, mask_image, get_images_and_labels_paths\n",
    "\n",
    "class InpaintLoraDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
    "    It pre-processes the images and the tokenizes prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_data_root,\n",
    "        tokenizer,\n",
    "        label_mapping: dict,\n",
    "        global_caption: Optional[str] = None,\n",
    "        size=512,\n",
    "        normalize=True,\n",
    "        augmentation=True,\n",
    "        scaling_pixels: int = 0,\n",
    "        labels_filter: Optional[list] = None,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        if not Path(instance_data_root).exists():\n",
    "            raise ValueError(\"Instance images root doesn't exists.\")\n",
    "\n",
    "        # Prepare the instance images and masks\n",
    "        self.imgs, self.labels = get_images_and_labels_paths(instance_data_root)\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "        self.global_caption = global_caption\n",
    "\n",
    "        self._length = len(self.imgs)\n",
    "\n",
    "        self.normalize = normalize\n",
    "        self.scaling_pixels = scaling_pixels\n",
    "        self.labels_filter = labels_filter # TODO implement labels filter\n",
    "        \n",
    "        self.mean, self.std = self.calculate_mean_std()\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size=self.size),\n",
    "                transforms.ToImageTensor(),\n",
    "                transforms.ConvertImageDtype(torch.float32),\n",
    "                transforms.Normalize(mean=self.mean, std=self.std)\n",
    "                if self.normalize\n",
    "                else transforms.Lambda(lambda x: x),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.mask_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size=self.size),\n",
    "                transforms.PILToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def calculate_mean_std(self):\n",
    "        means = []\n",
    "        stds = []\n",
    "\n",
    "        for img in self.imgs:\n",
    "            # calculate the mean and std of all the images\n",
    "            img = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img / 255.0\n",
    "            mean = np.mean(img, axis=(0, 1))\n",
    "            std = np.std(img, axis=(0, 1))\n",
    "            means.append(mean)\n",
    "            stds.append(std)\n",
    "\n",
    "        mean = np.mean(means, axis=0)\n",
    "        std = np.mean(stds, axis=0)\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "    def transform(self, image, mask):\n",
    "        \n",
    "        mask = torch.from_numpy(mask).unsqueeze(0)\n",
    "\n",
    "        image = self.image_transforms(image)\n",
    "        mask = self.mask_transforms(mask)\n",
    "\n",
    "        if self.augmentation:\n",
    "\n",
    "            while True:\n",
    "                # Random crop\n",
    "                i, j, h, w = transforms.RandomCrop.get_params(\n",
    "                    image, output_size=(self.size, self.size))\n",
    "                image = TF.crop(image, i, j, h, w)\n",
    "                mask = TF.crop(mask, i, j, h, w)\n",
    "\n",
    "                # Check if the mask contains at least one non-zero value\n",
    "                # TODO find a bug in the mask generation\n",
    "                # because sometimes the mask is all zeros\n",
    "                # when the size is not 512 (e.g. 768)\n",
    "                if torch.sum(mask) > 0:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # Random horizontal flipping\n",
    "            if random.random() > 0.5:\n",
    "                image = TF.hflip(image)\n",
    "                mask = TF.hflip(mask)\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "\n",
    "        image = Image.open(self.imgs[index])\n",
    "        polygons = parse_labels(self.labels[index])\n",
    "        scaled_polygons = scale_polygons(polygons, image.size)\n",
    "        mask, label = generate_masks(\n",
    "            scaled_polygons, \n",
    "            image.size, \n",
    "            scaling_pixels=self.scaling_pixels\n",
    "        )\n",
    "        image, mask = self.transform(image, mask)\n",
    "\n",
    "\n",
    "        example[\"instance_images\"] = image\n",
    "        example[\"instance_masks\"] = mask\n",
    "        example[\"instance_masked_images\"] = mask_image(image, example[\"instance_masks\"], invert=True)\n",
    "        example[\"instance_masked_values\"] = mask_image(image, example[\"instance_masks\"], invert=False)\n",
    "        \n",
    "        text = self.label_mapping[label]\n",
    "        if self.global_caption:\n",
    "            text += ', ' + self.global_caption.strip()\n",
    "\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "        ).input_ids\n",
    "\n",
    "        return example\n",
    "\n",
    "class InpaintingDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, tokenizer, device, batch_size=1, **kwargs):\n",
    "        super().__init__(dataset, collate_fn=self.collate_fn, batch_size=batch_size, **kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def collate_fn(self, examples):\n",
    "        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "        pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "        mask_values = [example[\"instance_masked_values\"] for example in examples]\n",
    "        masked_image_values = [\n",
    "            example[\"instance_masked_images\"] for example in examples\n",
    "        ]\n",
    "        mask = [example[\"instance_masks\"] for example in examples]\n",
    "\n",
    "        # Concat class and instance examples for prior preservation.\n",
    "        # We do this to avoid doing two forward passes.\n",
    "        if examples[0].get(\"class_prompt_ids\", None) is not None:\n",
    "            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
    "            pixel_values += [example[\"class_images\"] for example in examples]\n",
    "            mask_values += [example[\"class_masks\"] for example in examples]\n",
    "            masked_image_values += [\n",
    "                example[\"class_masked_images\"] for example in examples\n",
    "            ]\n",
    "\n",
    "        pixel_values = (\n",
    "            torch.stack(pixel_values).to(memory_format=torch.contiguous_format).float()\n",
    "        )\n",
    "        mask_values = (\n",
    "            torch.stack(mask_values).to(memory_format=torch.contiguous_format).float()\n",
    "        )\n",
    "        masked_image_values = (\n",
    "            torch.stack(masked_image_values).to(memory_format=torch.contiguous_format).float()\n",
    "        )\n",
    "        mask = (\n",
    "            torch.stack(mask).to(memory_format=torch.contiguous_format).float()\n",
    "        )\n",
    "\n",
    "        input_ids = self.tokenizer.pad(\n",
    "            {\"input_ids\": input_ids},\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"mask_values\": mask_values,\n",
    "            \"masked_image_values\": masked_image_values,\n",
    "            \"mask\": mask,\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "def download_roboflow_dataset(config):\n",
    "    rf = Roboflow(api_key=config.dataset.roboflow_api_key)\n",
    "    project = rf.workspace(config.dataset.roboflow_workspace).project(config.dataset.project_name)\n",
    "    dataset = project.version(config.dataset.dataset_version).download(\"yolov7\", location=config.dataset.data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaidacundo\u001b[0m (\u001b[33marked\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Facundo\\Desktop\\ARKED\\training\\inpainting_training\\lora_inpainting\\testing_notebooks\\wandb\\run-20231114_111808-5fkznn9d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arked/facades/runs/5fkznn9d' target=\"_blank\">clean-butterfly-11</a></strong> to <a href='https://wandb.ai/arked/facades' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arked/facades' target=\"_blank\">https://wandb.ai/arked/facades</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arked/facades/runs/5fkznn9d' target=\"_blank\">https://wandb.ai/arked/facades/runs/5fkznn9d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: unet\n",
      "trainable params: 2492928 || all params: 862028292 || trainable%: 0.28919329250970804\n",
      "Model: text_encoder\n",
      "trainable params: 1327104 || all params: 124387584 || trainable%: 1.0669103437204794\n",
      "Directory 'facades_checkpoints' created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "label_mapping = get_label_mapping(os.path.join(config.dataset.data_root, \"data.yaml\"))\n",
    "\n",
    "train_dataset = InpaintLoraDataset(\n",
    "    instance_data_root= os.path.join(config.dataset.data_root, \"train\"),\n",
    "    tokenizer=tokenizer,\n",
    "    label_mapping=label_mapping,\n",
    "    global_caption=config.prompt.global_caption,\n",
    "    size=config.dataset.image_size,\n",
    "    normalize=config.dataset.normalize_images,\n",
    "    scaling_pixels=config.dataset.scaling_pixels,\n",
    ")\n",
    "\n",
    "valid_dataset = InpaintLoraDataset(\n",
    "    instance_data_root=os.path.join(config.dataset.data_root, \"valid\"),\n",
    "    tokenizer=tokenizer,\n",
    "    label_mapping=label_mapping,\n",
    "    global_caption=config.prompt.global_caption,\n",
    "    size=config.dataset.image_size,\n",
    "    normalize=False,\n",
    "    augmentation=False,\n",
    ")\n",
    "\n",
    "train_dataloader = InpaintingDataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.train.train_batch_size,\n",
    "    tokenizer=tokenizer,\n",
    "    device=config.device,\n",
    ")\n",
    "valid_dataloader = InpaintingDataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.train.eval_batch_size,\n",
    "    tokenizer=tokenizer,\n",
    "    device=config.device,\n",
    ")\n",
    "\n",
    "# Freeze all weights\n",
    "unet.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "if config.train.gradient_checkpointing:\n",
    "    unet.enable_gradient_checkpointing()\n",
    "\n",
    "if config.train.use_xformers and is_xformers_available():\n",
    "    unet.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "params_to_freeze = itertools.chain(\n",
    "    text_encoder.text_model.encoder.parameters(),\n",
    "    text_encoder.text_model.final_layer_norm.parameters(),\n",
    "    text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    ")\n",
    "for param in params_to_freeze:\n",
    "    param.requires_grad = False\n",
    "\n",
    "if config.log_wandb:\n",
    "    wandb.init(\n",
    "        project=config.wandb.project_name, \n",
    "        entity=config.wandb.entity_name,\n",
    "        config=config,\n",
    "        id=config.wandb.run_name,\n",
    "        tags=config.wandb.tags,\n",
    "    )\n",
    "    wandb.watch(unet)\n",
    "    wandb.watch(text_encoder)\n",
    "\n",
    "if config.train.train_unet:\n",
    "    unet_peft = LoraConfig(\n",
    "        r=config.lora.rank,\n",
    "        lora_alpha=config.lora.alpha,\n",
    "        target_modules=config.lora.unet_target_modules,\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "    )\n",
    "\n",
    "    unet = LoraModel(unet, unet_peft, config.lora.unet_adapter_name)\n",
    "    print_trainable_parameters(unet, \"unet\")\n",
    "\n",
    "    params_to_optimize = [\n",
    "        {\n",
    "            \"params\": itertools.chain(unet.parameters()),\n",
    "            \"lr\": config.train.unet_lr,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "if config.train.train_text_encoder:\n",
    "    text_encoder_peft = LoraConfig(\n",
    "        r=config.lora.rank,\n",
    "        lora_alpha=config.lora.alpha,\n",
    "        target_modules=config.lora.text_encoder_target_modules,\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "    )\n",
    "\n",
    "    text_encoder = LoraModel(text_encoder, text_encoder_peft, config.lora.text_encoder_adapter_name)\n",
    "    print_trainable_parameters(text_encoder, \"text_encoder\")\n",
    "\n",
    "    params_to_optimize += [\n",
    "        {\n",
    "            \"params\": itertools.chain(text_encoder.parameters()),\n",
    "            \"lr\": config.train.text_encoder_lr,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "optimizer_lora = optim.AdamW(\n",
    "    params_to_optimize,\n",
    "    lr=config.train.learning_rate,\n",
    "    weight_decay=config.train.weight_decay,\n",
    ")\n",
    "\n",
    "if config.train.train_unet:\n",
    "    unet.train()\n",
    "    \n",
    "if config.train.train_text_encoder:\n",
    "    text_encoder.train()\n",
    "\n",
    "lr_scheduler_lora = get_scheduler(\n",
    "    config.train.scheduler_type,\n",
    "    optimizer=optimizer_lora,\n",
    "    num_warmup_steps=config.train.scheduler_warmup_steps,\n",
    "    num_training_steps=config.train.total_steps,\n",
    "    num_cycles=config.train.scheduler_num_cycles,\n",
    ")\n",
    "\n",
    "if not os.path.exists(config.train.checkpoint_folder):\n",
    "    os.makedirs(config.train.checkpoint_folder)\n",
    "    print(f\"Directory '{config.train.checkpoint_folder}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{config.train.checkpoint_folder}' already exists.\")\n",
    "\n",
    "loss_sum = 0.0\n",
    "progress_bar = tqdm(range(config.train.total_steps))\n",
    "progress_bar.set_description(\"Steps\")\n",
    "global_step = 0\n",
    "\n",
    "if config.train.text_encoder_train_ratio < 1.0:\n",
    "    text_encoder_steps = math.ceil(config.train.total_steps * config.train.text_encoder_train_ratio)\n",
    "else:\n",
    "    text_encoder_steps = config.train.total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InpaintLoraDataset(\n",
    "    instance_data_root= os.path.join(config.dataset.data_root, \"train\"),\n",
    "    tokenizer=tokenizer,\n",
    "    label_mapping=label_mapping,\n",
    "    global_caption=config.prompt.global_caption,\n",
    "    size=config.dataset.image_size,\n",
    "    normalize=False,\n",
    "    scaling_pixels=config.dataset.scaling_pixels,\n",
    ")\n",
    "\n",
    "valid_dataset = InpaintLoraDataset(\n",
    "    instance_data_root=os.path.join(config.dataset.data_root, \"valid\"),\n",
    "    tokenizer=tokenizer,\n",
    "    label_mapping=label_mapping,\n",
    "    global_caption=config.prompt.global_caption,\n",
    "    size=config.dataset.image_size,\n",
    "    normalize=False,\n",
    "    augmentation=False,\n",
    ")\n",
    "\n",
    "train_dataloader = InpaintingDataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.train.train_batch_size,\n",
    "    tokenizer=tokenizer,\n",
    "    device=config.device,\n",
    ")\n",
    "valid_dataloader = InpaintingDataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=config.train.eval_batch_size,\n",
    "    tokenizer=tokenizer,\n",
    "    device=config.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'pixel_values', 'mask_values', 'masked_image_values', 'mask'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_lora.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_step(\n",
    "    batch,\n",
    "    unet,\n",
    "    vae,\n",
    "    text_encoder,\n",
    "    scheduler,\n",
    "    t_mutliplier=1.0,\n",
    "    mixed_precision=False,\n",
    "    mask_temperature=1.0,\n",
    "    vae_scale_factor=8,\n",
    "    criterion='mse',\n",
    "):\n",
    "    weight_dtype = torch.float32\n",
    "\n",
    "    # encode the image\n",
    "    latents = vae.encode(\n",
    "            batch[\"pixel_values\"].to(dtype=weight_dtype).to(unet.device)\n",
    "        ).latent_dist.sample()\n",
    "\n",
    "\n",
    "    # encode the masked image\n",
    "    masked_image_latents = vae.encode(\n",
    "                batch[\"masked_image_values\"].to(dtype=weight_dtype).to(unet.device)\n",
    "            ).latent_dist.sample()\n",
    "    masked_image_latents = masked_image_latents * vae.config.scaling_factor\n",
    "    latents = latents * vae.config.scaling_factor\n",
    "\n",
    "    # scale the mask\n",
    "    mask = F.interpolate(\n",
    "                batch[\"mask\"].to(dtype=weight_dtype).to(unet.device),\n",
    "                scale_factor=1 / 8,\n",
    "            )\n",
    "\n",
    "    noise = torch.randn_like(latents)\n",
    "    bsz = latents.shape[0]\n",
    "\n",
    "    timesteps = torch.randint(\n",
    "        0,\n",
    "        int(scheduler.config.num_train_timesteps * t_mutliplier),\n",
    "        (bsz,),\n",
    "        device=latents.device,\n",
    "    )\n",
    "    timesteps = timesteps.long()\n",
    "\n",
    "    noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "    latent_model_input = torch.cat(\n",
    "            [noisy_latents, mask, masked_image_latents], dim=1\n",
    "        )\n",
    "\n",
    "    if mixed_precision:\n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            encoder_hidden_states = text_encoder(\n",
    "                batch[\"input_ids\"].to(text_encoder.device)\n",
    "            )[0]\n",
    "\n",
    "            model_pred = unet(latent_model_input, timesteps, encoder_hidden_states).sample\n",
    "    else:\n",
    "        encoder_hidden_states = text_encoder(\n",
    "            batch[\"input_ids\"].to(text_encoder.device)\n",
    "        )[0]\n",
    "\n",
    "        model_pred = unet(latent_model_input, timesteps, encoder_hidden_states).sample\n",
    "    if scheduler.config.prediction_type == \"epsilon\":\n",
    "        target = noise\n",
    "    elif scheduler.config.prediction_type == \"v_prediction\":\n",
    "        target = scheduler.get_velocity(latents, noise, timesteps)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown prediction type {scheduler.config.prediction_type}\")\n",
    "\n",
    "    if batch.get(\"mask\", None) is not None:\n",
    "\n",
    "        mask = (\n",
    "            batch[\"mask\"]\n",
    "            .to(model_pred.device)\n",
    "            .reshape(\n",
    "                model_pred.shape[0], 1, model_pred.shape[2] * vae_scale_factor, model_pred.shape[3] * vae_scale_factor\n",
    "            )\n",
    "        )\n",
    "        # resize to match model_pred\n",
    "        mask = F.interpolate(\n",
    "            mask.float(),\n",
    "            size=model_pred.shape[-2:],\n",
    "            mode=\"nearest\",\n",
    "        )\n",
    "\n",
    "        mask = mask.pow(mask_temperature)\n",
    "\n",
    "        mask = mask / mask.max()\n",
    "\n",
    "        model_pred = model_pred * mask\n",
    "\n",
    "        target = target * mask\n",
    "\n",
    "    if criterion == 'mse':\n",
    "        loss = (\n",
    "            F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
    "            .mean([1, 2, 3])\n",
    "            .mean()\n",
    "        )\n",
    "    elif criterion == 'ssim':\n",
    "        loss = (\n",
    "            ssim(model_pred.float(), target.float(), data_range=1, size_average=False)\n",
    "        )\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mutliplier=0.8\n",
    "scheduler = noise_scheduler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 6.00 GiB total capacity; 5.03 GiB already allocated; 0 bytes free; 5.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Facundo\\Desktop\\ARKED\\training\\inpainting_training\\lora_inpainting\\testing_notebooks\\ssim_testing.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Facundo/Desktop/ARKED/training/inpainting_training/lora_inpainting/testing_notebooks/ssim_testing.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m weight_dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat32\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Facundo/Desktop/ARKED/training/inpainting_training/lora_inpainting/testing_notebooks/ssim_testing.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# encode the image\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Facundo/Desktop/ARKED/training/inpainting_training/lora_inpainting/testing_notebooks/ssim_testing.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m latents \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39;49mencode(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Facundo/Desktop/ARKED/training/inpainting_training/lora_inpainting/testing_notebooks/ssim_testing.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         batch[\u001b[39m\"\u001b[39;49m\u001b[39mpixel_values\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mweight_dtype)\u001b[39m.\u001b[39;49mto(unet\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Facundo/Desktop/ARKED/training/inpainting_training/lora_inpainting/testing_notebooks/ssim_testing.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     )\u001b[39m.\u001b[39mlatent_dist\u001b[39m.\u001b[39msample()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Facundo/Desktop/ARKED/training/inpainting_training/lora_inpainting/testing_notebooks/ssim_testing.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# encode the masked image\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Facundo/Desktop/ARKED/training/inpainting_training/lora_inpainting/testing_notebooks/ssim_testing.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m masked_image_latents \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39mencode(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Facundo/Desktop/ARKED/training/inpainting_training/lora_inpainting/testing_notebooks/ssim_testing.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             batch[\u001b[39m\"\u001b[39m\u001b[39mmasked_image_values\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mweight_dtype)\u001b[39m.\u001b[39mto(unet\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Facundo/Desktop/ARKED/training/inpainting_training/lora_inpainting/testing_notebooks/ssim_testing.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         )\u001b[39m.\u001b[39mlatent_dist\u001b[39m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\Facundo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\diffusers\\utils\\accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_hf_hook\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hf_hook, \u001b[39m\"\u001b[39m\u001b[39mpre_forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpre_forward(\u001b[39mself\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Facundo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\diffusers\\models\\autoencoder_kl.py:274\u001b[0m, in \u001b[0;36mAutoencoderKL.encode\u001b[1;34m(self, x, return_dict)\u001b[0m\n\u001b[0;32m    272\u001b[0m     h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(encoded_slices)\n\u001b[0;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    276\u001b[0m moments \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_conv(h)\n\u001b[0;32m    277\u001b[0m posterior \u001b[39m=\u001b[39m DiagonalGaussianDistribution(moments)\n",
      "File \u001b[1;32mc:\\Users\\Facundo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Facundo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\diffusers\\models\\vae.py:165\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[39m# down\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39mfor\u001b[39;00m down_block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_blocks:\n\u001b[1;32m--> 165\u001b[0m         sample \u001b[39m=\u001b[39m down_block(sample)\n\u001b[0;32m    167\u001b[0m     \u001b[39m# middle\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmid_block(sample)\n",
      "File \u001b[1;32mc:\\Users\\Facundo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Facundo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\diffusers\\models\\unet_2d_blocks.py:1323\u001b[0m, in \u001b[0;36mDownEncoderBlock2D.forward\u001b[1;34m(self, hidden_states, scale)\u001b[0m\n\u001b[0;32m   1321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mFloatTensor, scale: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[0;32m   1322\u001b[0m     \u001b[39mfor\u001b[39;00m resnet \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresnets:\n\u001b[1;32m-> 1323\u001b[0m         hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, scale\u001b[39m=\u001b[39;49mscale)\n\u001b[0;32m   1325\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1326\u001b[0m         \u001b[39mfor\u001b[39;00m downsampler \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsamplers:\n",
      "File \u001b[1;32mc:\\Users\\Facundo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Facundo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\diffusers\\models\\resnet.py:693\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[1;34m(self, input_tensor, temb, scale)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    691\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(hidden_states)\n\u001b[1;32m--> 693\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnonlinearity(hidden_states)\n\u001b[0;32m    695\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    696\u001b[0m     \u001b[39m# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\u001b[39;00m\n\u001b[0;32m    697\u001b[0m     \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m64\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Facundo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Facundo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:396\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 396\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49msilu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\Facundo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2059\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2057\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   2058\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39msilu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m-> 2059\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49msilu(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 6.00 GiB total capacity; 5.03 GiB already allocated; 0 bytes free; 5.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "weight_dtype = torch.float32\n",
    "\n",
    "# encode the image\n",
    "latents = vae.encode(\n",
    "        batch[\"pixel_values\"].to(dtype=weight_dtype).to(unet.device)\n",
    "    ).latent_dist.sample()\n",
    "\n",
    "\n",
    "# encode the masked image\n",
    "masked_image_latents = vae.encode(\n",
    "            batch[\"masked_image_values\"].to(dtype=weight_dtype).to(unet.device)\n",
    "        ).latent_dist.sample()\n",
    "masked_image_latents = masked_image_latents * vae.config.scaling_factor\n",
    "latents = latents * vae.config.scaling_factor\n",
    "\n",
    "# scale the mask\n",
    "mask = F.interpolate(\n",
    "            batch[\"mask\"].to(dtype=weight_dtype).to(unet.device),\n",
    "            scale_factor=1 / 8,\n",
    "        )\n",
    "\n",
    "noise = torch.randn_like(latents)\n",
    "bsz = latents.shape[0]\n",
    "\n",
    "timesteps = torch.randint(\n",
    "    0,\n",
    "    int(scheduler.config.num_train_timesteps * t_mutliplier),\n",
    "    (bsz,),\n",
    "    device=latents.device,\n",
    ")\n",
    "timesteps = timesteps.long()\n",
    "\n",
    "noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "latent_model_input = torch.cat(\n",
    "        [noisy_latents, mask, masked_image_latents], dim=1\n",
    "    )\n",
    "\n",
    "if mixed_precision:\n",
    "    with torch.cuda.amp.autocast():\n",
    "\n",
    "        encoder_hidden_states = text_encoder(\n",
    "            batch[\"input_ids\"].to(text_encoder.device)\n",
    "        )[0]\n",
    "\n",
    "        model_pred = unet(latent_model_input, timesteps, encoder_hidden_states).sample\n",
    "else:\n",
    "    encoder_hidden_states = text_encoder(\n",
    "        batch[\"input_ids\"].to(text_encoder.device)\n",
    "    )[0]\n",
    "\n",
    "    model_pred = unet(latent_model_input, timesteps, encoder_hidden_states).sample\n",
    "if scheduler.config.prediction_type == \"epsilon\":\n",
    "    target = noise\n",
    "elif scheduler.config.prediction_type == \"v_prediction\":\n",
    "    target = scheduler.get_velocity(latents, noise, timesteps)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown prediction type {scheduler.config.prediction_type}\")\n",
    "\n",
    "if batch.get(\"mask\", None) is not None:\n",
    "\n",
    "    mask = (\n",
    "        batch[\"mask\"]\n",
    "        .to(model_pred.device)\n",
    "        .reshape(\n",
    "            model_pred.shape[0], 1, model_pred.shape[2] * vae_scale_factor, model_pred.shape[3] * vae_scale_factor\n",
    "        )\n",
    "    )\n",
    "    # resize to match model_pred\n",
    "    mask = F.interpolate(\n",
    "        mask.float(),\n",
    "        size=model_pred.shape[-2:],\n",
    "        mode=\"nearest\",\n",
    "    )\n",
    "\n",
    "    mask = mask.pow(mask_temperature)\n",
    "\n",
    "    mask = mask / mask.max()\n",
    "\n",
    "    model_pred = model_pred * mask\n",
    "\n",
    "    target = target * mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_lora = loss_step(\n",
    "    batch,\n",
    "    unet,\n",
    "    vae,\n",
    "    text_encoder,\n",
    "    noise_scheduler,\n",
    "    t_mutliplier=0.8,\n",
    "    mixed_precision=True,\n",
    "    mask_temperature=config.train.mask_temperature,\n",
    "    criterion='ssim',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_lora.backward()\n",
    "loss_sum += loss_lora.detach().item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
