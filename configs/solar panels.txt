from src.config import DatasetConfig, Config, ModelConfig, WandbConfig, EvaluationConfig, TrainConfig, LoraConfig

dataset_config = DatasetConfig(
    roboflow_api_key='HNXIsW3WwnidNDQZHexX',
    roboflow_workspace='arked',
    project_name='solar_panels-k7our',
    dataset_version=5,
    data_root='solar_panels_2',
    image_size=512,
    normalize_images=False,
)

model_config = ModelConfig(
    model_path=realistic_vision_path,
    vae_path=vae_path,
)

wandb_config = WandbConfig(
    project_name='solar_panels',
    run_name='proj-in_conv1-2_rank_4_mse+ssim'
) 

eval_config=EvaluationConfig(
    prompts=['solar panels'],
    num_images_per_prompt=12,
    compute_dino_score=True,
)

lora_config=LoraConfig(
    rank=4,
    alpha=32,
    unet_target_modules=["proj_in", "conv1", "conv2"],
    text_encoder_target_modules=["q_proj", "v_proj"],
    # "to_q", "to_v", "to_k", "to_out.0" are the names of the modules in attention layers
    # "ff.net.0.proj" is the name of the linear in the GEGLU activation
    # "proj_in", "conv1", "conv2" are the names of the modules in the resnet block
    # TODO understand better the role of each module and if it is needed to add more

    # "q_proj", "v_proj", "k_proj", "out_proj" are the names of the modules in the text encoder attention layers
    # "mlp.fc1", "mlp.fc2" are the names of the modules in the text encoder mlp that produce the embeddings
    output_format='peft', # can be either 'kohya_ss' or 'peft', 'kohya_ss' is compatible with A1111
)

train_config=TrainConfig(
    checkpoint_folder=wandb_config.project_name + "_checkpoints",
    train_batch_size=4,
    unet_lr=1e-4,
    text_encoder_lr=1e-4,
    scheduler_num_cycles=4,
    total_steps=4000,
    criterion='mse+ssim'
)

config = Config(
    dataset=dataset_config,
    model=model_config,
    wandb=wandb_config,
    eval=eval_config,
    train=train_config,
    lora=lora_config,
)