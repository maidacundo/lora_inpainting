{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q roboflow diffusers transformers accelerate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"\")\n",
    "project = rf.workspace(\"arked\").project(\"white-facade\")\n",
    "\n",
    "VERSION = 9\n",
    "\n",
    "dataset = project.version(VERSION).download(\"yolov7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as path\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to parse the text label and extract polygon information\n",
    "def parse_labels(label_file):\n",
    "    with open(label_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    polygons = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) >= 5:\n",
    "            class_id = int(parts[0])\n",
    "            coordinates = [float(coord) for coord in parts[1:]]\n",
    "            num_points = len(coordinates) // 2  # Number of (x, y) pairs\n",
    "            polygon = [(coordinates[i], coordinates[i + 1]) for i in range(0, len(coordinates), 2)]\n",
    "            polygons.append((class_id, polygon))\n",
    "\n",
    "    return polygons\n",
    "\n",
    "# Function to plot bounding boxes\n",
    "def plot_polygons(image_path, polygons):\n",
    "    # Load the image if needed\n",
    "    image = plt.imread(image_path)\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "\n",
    "    # Generate unique colors for classes\n",
    "    class_ids = set([class_id for class_id, _ in polygons])\n",
    "    num_classes = len(class_ids)\n",
    "    colors = generate_colors(num_classes)\n",
    "\n",
    "    # Plot each polygon\n",
    "    for class_id, polygon in polygons:\n",
    "        x, y = zip(*polygon)\n",
    "        color = colors[class_id % num_classes]\n",
    "        polygon = plt.Polygon(np.c_[x, y], linewidth=1, edgecolor=color, facecolor='none', label=f'Class {class_id}')\n",
    "        ax.add_patch(polygon)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(image)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    # ax.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Function to generate unique colors based on class ID\n",
    "def generate_colors(num_classes):\n",
    "    colormap = plt.cm.get_cmap('tab20', num_classes)\n",
    "    colors = [colormap(i) for i in range(num_classes)]\n",
    "    return colors\n",
    "\n",
    "# Function to scale polygons to match an image size\n",
    "def scale_polygons(polygons, image_size):\n",
    "    scaled_polygons = []\n",
    "\n",
    "    for class_id, polygon in polygons:\n",
    "        # Scale the polygon coordinates\n",
    "        scaled_polygon = [(x * image_size[0], y * image_size[1]) for x, y in polygon]\n",
    "        scaled_polygons.append((class_id, scaled_polygon))\n",
    "\n",
    "    return scaled_polygons\n",
    "\n",
    "def enlarge_mask(mask, scaling_pixels=1):\n",
    "    enlarged_mask = mask.copy()\n",
    "\n",
    "    for s in range(1, scaling_pixels+1):\n",
    "        # Create masks for shifting in all four directions\n",
    "        up_mask = np.roll(mask, s, axis=0)\n",
    "        down_mask = np.roll(mask, -s, axis=0)\n",
    "        left_mask = np.roll(mask, s, axis=1)\n",
    "        right_mask = np.roll(mask, -s, axis=1)\n",
    "\n",
    "        # Use logical OR to combine the shifted masks\n",
    "        enlarged_mask = np.logical_or(enlarged_mask, up_mask)\n",
    "        enlarged_mask = np.logical_or(enlarged_mask, down_mask)\n",
    "        enlarged_mask = np.logical_or(enlarged_mask, left_mask)\n",
    "        enlarged_mask = np.logical_or(enlarged_mask, right_mask)\n",
    "\n",
    "    return enlarged_mask.astype(np.uint8)\n",
    "\n",
    "# generate mask from polygons\n",
    "def generate_masks(polygons, image_size, label, num_samples=1, scaling_pixels=None):\n",
    "    mask = np.zeros(image_size, dtype=np.uint8)\n",
    "\n",
    "    i=0\n",
    "    for class_id, polygon in polygons:\n",
    "        if (class_id == label):\n",
    "            x, y = zip(*polygon)\n",
    "            # Create a Path object from the polygon coordinates\n",
    "            path_polygon = path.Path(list(zip(x, y)))\n",
    "\n",
    "            # Generate a mask for the polygon\n",
    "            x, y = np.meshgrid(np.arange(image_size[0]), np.arange(image_size[1]))\n",
    "            x, y = x.flatten(), y.flatten()\n",
    "            points = np.vstack((x, y)).T\n",
    "            mask_indices = path_polygon.contains_points(points).reshape(image_size[0], image_size[1])\n",
    "            # Fill the polygon in the mask\n",
    "            mask[mask_indices] = 1\n",
    "\n",
    "            i+=1\n",
    "            if (i == num_samples):\n",
    "                break\n",
    "\n",
    "    if scaling_pixels:\n",
    "        mask = enlarge_mask(mask, scaling_pixels)\n",
    "        return mask\n",
    "    return mask\n",
    "\n",
    "def create_dataset(image_paths, label_paths, label, num_samples=1, resize_shape=None, scaling_pixels=None):\n",
    "    images = []\n",
    "    masks = []\n",
    "\n",
    "    # list of all the images in the image folder\n",
    "    images_names = os.listdir(image_paths)\n",
    "    label_names = [image_name.replace(\".jpg\", \".txt\") for image_name in images_names]\n",
    "    for img_name, label_name in zip(images_names, label_names):\n",
    "        if (label_name not in os.listdir(label_paths)):\n",
    "            images_names.remove(img_name)\n",
    "            label_names.remove(label_name)\n",
    "        img_path = image_paths + img_name\n",
    "        label_path = label_paths + label_name\n",
    "\n",
    "        # read the rgb image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        image_size = (img.shape[0], img.shape[1])\n",
    "        polygons = parse_labels(label_path)\n",
    "        scaled_polygons = scale_polygons(polygons, image_size)\n",
    "        mask = generate_masks(scaled_polygons, image_size, label=label, num_samples=num_samples, scaling_pixels=scaling_pixels)\n",
    "\n",
    "        # resize the image\n",
    "        if resize_shape is not None:\n",
    "            img = cv2.resize(img, resize_shape, interpolation=cv2.INTER_AREA)\n",
    "            mask = cv2.resize(mask, resize_shape, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        if len(np.unique(mask)) > 1:\n",
    "            images.append(img)\n",
    "            masks.append(mask)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "# mask the images with the masks\n",
    "def mask_images(images, masks, invert=False):\n",
    "    masked_images = []\n",
    "    for img, mask in zip(images, masks):\n",
    "        if invert:\n",
    "            masked_images.append(img * (1 - mask[..., np.newaxis]))\n",
    "        else:\n",
    "            masked_images.append(img * mask[..., np.newaxis])\n",
    "    return np.array(masked_images)\n",
    "\n",
    "def get_label_mapping(data_yaml):\n",
    "    with open(data_yaml, 'r') as file:\n",
    "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    label_mapping = {}\n",
    "    for i, label in enumerate(data['names']):\n",
    "        label_mapping[label] = i\n",
    "\n",
    "    return label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"White-Facade-2\"\n",
    "\n",
    "train_img_path = PATH + \"/train/images/\"\n",
    "train_label_path = PATH + \"/train/labels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset\n",
    "train_images, train_masks = create_dataset(train_img_path, train_label_path, label=0, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the image with the mask\n",
    "\n",
    "plt.imshow(train_images[0])\n",
    "plt.imshow(train_masks[0], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    UNet2DConditionModel,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionPipeline,\n",
    ")\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from typing import List, Optional\n",
    "\n",
    "def get_models(\n",
    "    pretrained_model_name,\n",
    "    pretrained_vae_name_or_path: Optional[str] = None,\n",
    "    device: str = \"cuda:0\",\n",
    "):\n",
    "\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        pretrained_model_name,\n",
    "        subfolder=\"tokenizer\",\n",
    "    )\n",
    "\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        pretrained_model_name,\n",
    "        subfolder=\"text_encoder\",\n",
    "    )\n",
    "\n",
    "    placeholder_token_ids = []\n",
    "\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        pretrained_vae_name_or_path or pretrained_model_name,\n",
    "        subfolder=None if pretrained_vae_name_or_path else \"vae\",\n",
    "    )\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        pretrained_model_name,\n",
    "        subfolder=\"unet\",\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        text_encoder.to(device),\n",
    "        vae.to(device),\n",
    "        unet.to(device),\n",
    "        tokenizer,\n",
    "        placeholder_token_ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "class InpaintLoraDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
    "    It pre-processes the images and the tokenizes prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_data_root,\n",
    "        tokenizer,\n",
    "        label: int,\n",
    "        global_caption: str,\n",
    "        token_map: Optional[dict] = None,\n",
    "        size=512,\n",
    "        h_flip=True,\n",
    "        resize=True,\n",
    "        blur_amount: int = 70,\n",
    "        scaling_pixels: int = 5,\n",
    "        train_inpainting: bool = True,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.resize = resize\n",
    "        self.train_inpainting = train_inpainting\n",
    "        \n",
    "        if not Path(instance_data_root).exists():\n",
    "            raise ValueError(\"Instance images root doesn't exists.\")\n",
    "        img_path = instance_data_root + \"images/\"\n",
    "        label_path = instance_data_root + \"labels/\"\n",
    "\n",
    "        # Prepare the instance images and masks\n",
    "        self.imgs, self.masks = create_dataset(img_path, label_path, label, num_samples=2, scaling_pixels=scaling_pixels)\n",
    "        self.global_captions = global_caption\n",
    "\n",
    "        self.token_map = token_map\n",
    "        \n",
    "        self._length = len(self.imgs)\n",
    "\n",
    "        self.h_flip = h_flip\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [   \n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(\n",
    "                    size, interpolation=transforms.InterpolationMode.BILINEAR\n",
    "                )\n",
    "                if resize\n",
    "                else transforms.Lambda(lambda x: x),\n",
    "                transforms.CenterCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        self.mask_transforms = transforms.Compose(\n",
    "            [   \n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(\n",
    "                    size, interpolation=transforms.InterpolationMode.BILINEAR\n",
    "                )\n",
    "                if resize\n",
    "                else transforms.Lambda(lambda x: x),\n",
    "                transforms.CenterCrop(size),\n",
    "                transforms.PILToTensor()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.blur_amount = blur_amount\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "\n",
    "        if self.train_inpainting:\n",
    "            example[\"instance_masks\"] = self.masks[index]\n",
    "            example[\"instance_masked_images\"] = mask_images(self.imgs[index], example[\"instance_masks\"], invert=True)\n",
    "            example[\"instance_masked_values\"] = mask_images(self.imgs[index], example[\"instance_masks\"], invert=False)\n",
    "\n",
    "        example[\"instance_images\"] = self.image_transforms(self.imgs[index])\n",
    "        example[\"instance_masked_images\"] = self.image_transforms(example[\"instance_masked_images\"])\n",
    "        example[\"instance_masked_values\"] = self.image_transforms(example[\"instance_masked_values\"])\n",
    "        example[\"instance_masks\"] = self.mask_transforms(example[\"instance_masks\"])\n",
    "\n",
    "        \"\"\"\n",
    "        if self.use_template:\n",
    "            assert self.token_map is not None\n",
    "            input_tok = list(self.token_map.values())[0]\n",
    "\n",
    "            text = random.choice(self.templates).format(input_tok)\n",
    "        else:\n",
    "            text = self.global_captions[index].strip()\n",
    "\n",
    "            if self.token_map is not None: # TODO implement this\n",
    "                for token, value in self.token_map.items():\n",
    "                    text = text.replace(token, value)\n",
    "        \"\"\"\n",
    "\n",
    "        text = self.global_captions.strip()\n",
    "\n",
    "        if self.h_flip and random.random() > 0.5:\n",
    "            hflip = transforms.RandomHorizontalFlip(p=1)\n",
    "\n",
    "            example[\"instance_images\"] = hflip(example[\"instance_images\"])\n",
    "            example[\"instance_masked_images\"] = hflip(example[\"instance_masked_images\"])\n",
    "            example[\"instance_masked_values\"] = hflip(example[\"instance_masked_values\"])\n",
    "            example[\"instance_masks\"] = hflip(example[\"instance_masks\"])\n",
    "\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "        ).input_ids\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inpainting_dataloader(\n",
    "    train_dataset, train_batch_size, tokenizer\n",
    "):\n",
    "    def collate_fn(examples):\n",
    "        input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "        pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "        mask_values = [example[\"instance_masked_values\"] for example in examples]\n",
    "        masked_image_values = [\n",
    "            example[\"instance_masked_images\"] for example in examples\n",
    "        ]\n",
    "        mask = [example[\"instance_masks\"] for example in examples]\n",
    "\n",
    "        # Concat class and instance examples for prior preservation.\n",
    "        # We do this to avoid doing two forward passes.\n",
    "        if examples[0].get(\"class_prompt_ids\", None) is not None:\n",
    "            input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
    "            pixel_values += [example[\"class_images\"] for example in examples]\n",
    "            mask_values += [example[\"class_masks\"] for example in examples]\n",
    "            masked_image_values += [\n",
    "                example[\"class_masked_images\"] for example in examples\n",
    "            ]\n",
    "\n",
    "        pixel_values = (\n",
    "            torch.stack(pixel_values).to(memory_format=torch.contiguous_format).float()\n",
    "        )\n",
    "        mask_values = (\n",
    "            torch.stack(mask_values).to(memory_format=torch.contiguous_format).float()\n",
    "        )\n",
    "        masked_image_values = (\n",
    "            torch.stack(masked_image_values).to(memory_format=torch.contiguous_format).float()\n",
    "        )\n",
    "        mask = (\n",
    "            torch.stack(mask).to(memory_format=torch.contiguous_format).float()\n",
    "        )\n",
    "\n",
    "        input_ids = tokenizer.pad(\n",
    "            {\"input_ids\": input_ids},\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"mask_values\": mask_values,\n",
    "            \"masked_image_values\": masked_image_values,\n",
    "            \"mask\": mask,\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roof_label = 9\n",
    "pretrained_model_name = \"stabilityai/stable-diffusion-2-inpainting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder, vae, unet, tokenizer, placeholder_token_ids = get_models(\n",
    "    pretrained_model_name,\n",
    "    device=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDIMScheduler\n",
    "\n",
    "noise_scheduler = DDIMScheduler.from_pretrained(\n",
    "    pretrained_model_name, subfolder=\"scheduler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InpaintLoraDataset(\n",
    "    instance_data_root=\"Bounding-Boxes-3/train/\",\n",
    "    tokenizer=tokenizer,\n",
    "    label=roof_label,\n",
    "    global_caption=\"A roof\",\n",
    "    token_map=None,\n",
    "    size=512,\n",
    "    h_flip=False,\n",
    "    resize=True,\n",
    "    blur_amount=70,\n",
    "    scaling_pixels=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_dataset[0]\n",
    "\n",
    "# plot the mask on the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(example[\"instance_images\"].permute(1, 2, 0))\n",
    "plt.imshow(example[\"instance_masks\"].permute(1, 2, 0), alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = inpainting_dataloader(\n",
    "    train_dataset, train_batch_size=1, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# plot the batch\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.title(\"pixel_values\")\n",
    "plt.imshow(batch[\"pixel_values\"][0].permute(1, 2, 0))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.title(\"masked_image_values\")\n",
    "plt.imshow(batch[\"masked_image_values\"][0].permute(1, 2, 0))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.title(\"mask_values\")\n",
    "plt.imshow(batch[\"mask_values\"][0].permute(1, 2, 0))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.title(\"mask\")\n",
    "plt.imshow(batch[\"mask\"][0].permute(1, 2, 0))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def loss_step(\n",
    "    batch,\n",
    "    unet,\n",
    "    vae,\n",
    "    text_encoder,\n",
    "    scheduler,\n",
    "    t_mutliplier=1.0,\n",
    "    mixed_precision=False,\n",
    "    mask_temperature=1.0,\n",
    "    vae_scale_factor=8,\n",
    "):\n",
    "    weight_dtype = torch.float32\n",
    "\n",
    "    # encode the image\n",
    "    latents = vae.encode(\n",
    "            batch[\"pixel_values\"].to(dtype=weight_dtype).to(unet.device)\n",
    "        ).latent_dist.sample()\n",
    "    \n",
    "    \n",
    "    # encode the masked image\n",
    "    masked_image_latents = vae.encode(\n",
    "                batch[\"masked_image_values\"].to(dtype=weight_dtype).to(unet.device)\n",
    "            ).latent_dist.sample()\n",
    "    masked_image_latents = masked_image_latents * vae.config.scaling_factor\n",
    "    latents = latents * vae.config.scaling_factor\n",
    "    \n",
    "    # scale the mask\n",
    "    mask = F.interpolate(\n",
    "                batch[\"mask\"].to(dtype=weight_dtype).to(unet.device),\n",
    "                scale_factor=1 / 8,\n",
    "            )\n",
    "\n",
    "    noise = torch.randn_like(latents)\n",
    "    bsz = latents.shape[0]\n",
    "\n",
    "    timesteps = torch.randint(\n",
    "        0,\n",
    "        int(scheduler.config.num_train_timesteps * t_mutliplier),\n",
    "        (bsz,),\n",
    "        device=latents.device,\n",
    "    )\n",
    "    timesteps = timesteps.long()\n",
    "\n",
    "    noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "    latent_model_input = torch.cat(\n",
    "            [noisy_latents, mask, masked_image_latents], dim=1\n",
    "        )\n",
    "\n",
    "    if mixed_precision:\n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            encoder_hidden_states = text_encoder(\n",
    "                batch[\"input_ids\"].to(text_encoder.device)\n",
    "            )[0]\n",
    "\n",
    "            model_pred = unet(latent_model_input, timesteps, encoder_hidden_states).sample\n",
    "    else:\n",
    "        encoder_hidden_states = text_encoder(\n",
    "            batch[\"input_ids\"].to(text_encoder.device)\n",
    "        )[0]\n",
    "\n",
    "        model_pred = unet(latent_model_input, timesteps, encoder_hidden_states).sample\n",
    "    if scheduler.config.prediction_type == \"epsilon\":\n",
    "        target = noise\n",
    "    elif scheduler.config.prediction_type == \"v_prediction\":\n",
    "        target = scheduler.get_velocity(latents, noise, timesteps)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown prediction type {scheduler.config.prediction_type}\")\n",
    "\n",
    "    if batch.get(\"mask\", None) is not None:\n",
    "\n",
    "        mask = (\n",
    "            batch[\"mask\"]\n",
    "            .to(model_pred.device)\n",
    "            .reshape(\n",
    "                model_pred.shape[0], 1, model_pred.shape[2] * vae_scale_factor, model_pred.shape[3] * vae_scale_factor\n",
    "            )\n",
    "        )\n",
    "        # resize to match model_pred\n",
    "        mask = F.interpolate(\n",
    "            mask.float(),\n",
    "            size=model_pred.shape[-2:],\n",
    "            mode=\"nearest\",\n",
    "        )\n",
    "        \n",
    "        mask = (mask + 0.01).pow(mask_temperature)\n",
    "\n",
    "        mask = mask / mask.max()\n",
    "\n",
    "        model_pred = model_pred * mask\n",
    "\n",
    "        target = target * mask\n",
    "\n",
    "    loss = (\n",
    "        F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
    "        .mean([1, 2, 3])\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "weight_dtype = torch.float16\n",
    "\n",
    "unet_lr = 1e-5\n",
    "text_encoder_lr = 1e-4\n",
    "weight_decay_lora = 1e-6\n",
    "gradient_checkpointing = False\n",
    "mixed_precision = False\n",
    "use_xformers = True\n",
    "train_text_encoder = True\n",
    "\n",
    "unet.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "\n",
    "if gradient_checkpointing:\n",
    "    unet.enable_gradient_checkpointing()\n",
    "\n",
    "if use_xformers and is_xformers_available():\n",
    "    unet.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "params_to_freeze = itertools.chain(\n",
    "    text_encoder.text_model.encoder.parameters(),\n",
    "    text_encoder.text_model.final_layer_norm.parameters(),\n",
    "    text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    ")\n",
    "for param in params_to_freeze:\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import inject_trainable_lora\n",
    "from torch import optim\n",
    "from torch.cuda import amp\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "lora_unet_target_modules={\"CrossAttention\", \"Attention\", \"GEGLU\"}\n",
    "lora_dropout_p = 0.1\n",
    "lora_rank = 64\n",
    "lora_scale = 1.0\n",
    "\n",
    "unet_lora_params, _ = inject_trainable_lora(\n",
    "    unet,\n",
    "    r=lora_rank,\n",
    "    target_replace_module=lora_unet_target_modules,\n",
    "    dropout_p=lora_dropout_p,\n",
    "    scale=lora_scale,\n",
    ")\n",
    "\n",
    "params_to_optimize = [\n",
    "    {\"params\": itertools.chain(*unet_lora_params), \"lr\": unet_lr},\n",
    "]\n",
    "\n",
    "if mixed_precision:\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    params_to_optimize,\n",
    "    lr=unet_lr,\n",
    "    weight_decay=weight_decay_lora,\n",
    ")\n",
    "\n",
    "if mixed_precision:\n",
    "    unet, optimizer = amp.initialize(unet, optimizer, opt_level=\"O1\")\n",
    "\n",
    "unet.train()\n",
    "if train_text_encoder:\n",
    "    text_encoder.train()\n",
    "\n",
    "# TODO parametrize\n",
    "lr_warmup_steps = 100\n",
    "max_train_steps = 1000\n",
    "\n",
    "lr_scheduler_lora = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=lr_warmup_steps,\n",
    "    num_training_steps=max_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key='')\n",
    "wandb.init(project=\"inpainting-lora\")\n",
    "\n",
    "wandb.config.update(\n",
    "    {\n",
    "        \"unet_lr\": unet_lr,\n",
    "        \"text_encoder_lr\": text_encoder_lr,\n",
    "        \"weight_decay_lora\": weight_decay_lora,\n",
    "        \"gradient_checkpointing\": gradient_checkpointing,\n",
    "        \"mixed_precision\": mixed_precision,\n",
    "        \"use_xformers\": use_xformers,\n",
    "        \"train_text_encoder\": train_text_encoder,\n",
    "        \"lr_warmup_steps\": lr_warmup_steps,\n",
    "        \"max_train_steps\": max_train_steps,\n",
    "        \"lora_unet_target_modules\": lora_unet_target_modules,\n",
    "        \"lora_dropout_p\": lora_dropout_p,\n",
    "        \"lora_rank\": lora_rank,\n",
    "        \"lora_scale\": lora_scale,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "log_wandb = False\n",
    "\n",
    "num_steps = 1000\n",
    "save_steps = 100\n",
    "mask_temperature = 1.0\n",
    "\n",
    "loss_sum = 0.0\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "progress_bar.set_description(\"Steps\")\n",
    "global_step = 0\n",
    "\n",
    "loss_sum = 0.0\n",
    "\n",
    "for epoch in range(math.ceil(num_steps / len(train_dataloader))):\n",
    "    for batch in train_dataloader:\n",
    "        lr_scheduler_lora.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        loss = loss_step(\n",
    "            batch,\n",
    "            unet,\n",
    "            vae,\n",
    "            text_encoder,\n",
    "            noise_scheduler,\n",
    "            t_mutliplier=0.8,\n",
    "            mixed_precision=True,\n",
    "            mask_temperature=mask_temperature,\n",
    "        )\n",
    "        loss_sum += loss.detach().item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            itertools.chain(unet.parameters(), text_encoder.parameters()), 1.0\n",
    "        )\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)\n",
    "        logs = {\n",
    "            \"loss\": loss.detach().item(),\n",
    "            \"lr\": lr_scheduler_lora.get_last_lr()[0],\n",
    "        }\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        wandb.log(logs)\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % save_steps == 0:\n",
    "            # TODO save model\n",
    "            pass\n",
    "\n",
    "            if log_wandb:\n",
    "                with torch.no_grad():\n",
    "                    pass\n",
    "                    \"\"\"pipe = StableDiffusionPipeline(\n",
    "                        vae=vae,\n",
    "                        text_encoder=text_encoder,\n",
    "                        tokenizer=tokenizer,\n",
    "                        unet=unet,\n",
    "                        scheduler=noise_scheduler,\n",
    "                        safety_checker=None,\n",
    "                        feature_extractor=None,\n",
    "                    )\n",
    "\n",
    "                    # open all images in test_image_path\n",
    "                    images = []\n",
    "                    for file in os.listdir(test_image_path):\n",
    "                        if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
    "                            images.append(\n",
    "                                Image.open(os.path.join(test_image_path, file))\n",
    "                            )\n",
    "\n",
    "                    wandb.log({\"loss\": loss_sum / save_steps})\n",
    "                    loss_sum = 0.0\n",
    "                    wandb.log(\n",
    "                        evaluate_pipe(\n",
    "                            pipe,\n",
    "                            target_images=images,\n",
    "                            class_token=class_token,\n",
    "                            learnt_token=\"\".join(placeholder_tokens),\n",
    "                            n_test=wandb_log_prompt_cnt,\n",
    "                            n_step=50,\n",
    "                            clip_model_sets=preped_clip,\n",
    "                        )\n",
    "                    )\"\"\"\n",
    "\n",
    "        if global_step >= num_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import save_lora_weight\n",
    "\n",
    "save_path = \"lora_inpainting.pt\"\n",
    "target_replace_module_unet = {\"CrossAttention\", \"Attention\", \"GEGLU\"}\n",
    "save_lora_weight(\n",
    "    unet, save_path, target_replace_module=target_replace_module_unet\n",
    ")\n",
    "print(\"Unet saved to \", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "\n",
    "pipe = StableDiffusionInpaintPipeline(\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet,\n",
    "    scheduler=noise_scheduler,\n",
    "    safety_checker=None,\n",
    "    feature_extractor=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = InpaintLoraDataset(\n",
    "    instance_data_root=\"Bounding-Boxes-3/test/\",\n",
    "    tokenizer=tokenizer,\n",
    "    label=roof_label,\n",
    "    global_caption=\"A roof\",\n",
    "    token_map=None,\n",
    "    size=512,\n",
    "    h_flip=False,\n",
    "    color_jitter=False,\n",
    "    resize=True,\n",
    "    train_inpainting=True,\n",
    "    blur_amount=70,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_dataset[0][\"instance_images\"].permute(1, 2, 0))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(test_dataset[0][\"instance_masks\"].permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'A roof'\n",
    "image = test_dataset[0][\"instance_images\"]\n",
    "mask_image = test_dataset[0][\"instance_masks\"]\n",
    "\n",
    "generated_image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"models//whiteFacades-000008.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "lora_tensors = {}\n",
    "with safe_open(\"models//lora_weight.safetensors\", framework=\"pt\", device=0) as f:\n",
    "    for k in f.keys():\n",
    "        lora_tensors[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_tensors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loras = torch.load(\"models//whiteFacades-000008.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loras = torch.load(\"models//lora_inpainting (1).pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
